{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K95giWRV41qu",
        "outputId": "f9201da8-2acd-4f6c-d635-807ca6f62f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.17-py3-none-any.whl (867 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.5-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.36 (from langchain)\n",
            "  Downloading langchain_community-0.0.36-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.48 (from langchain)\n",
            "  Downloading langchain_core-0.1.50-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.54-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.48->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed dataclasses-json-0.6.5 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.17 langchain-community-0.0.36 langchain-core-0.1.50 langchain-text-splitters-0.0.1 langsmith-0.1.54 marshmallow-3.21.2 mypy-extensions-1.0.0 orjson-3.10.3 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "#install langchain\n",
        "!pip install langchain\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setup the env variable\n",
        "from google.colab import userdata\n",
        "huggingface_api=userdata.get('huggingface_api')"
      ],
      "metadata": {
        "id": "xGNcyi6o5y2u"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai_api=userdata.get('openai_api')"
      ],
      "metadata": {
        "id": "ElpETxDm7VsI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY']=openai_api\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN']=huggingface_api"
      ],
      "metadata": {
        "id": "AYnvjvc87aZp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install openai\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPpjuGs_8VeN",
        "outputId": "82bda8db-f702-49e6-b812-87e6a0f26403"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.25.1-py3-none-any.whl (312 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/312.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m225.3/312.9 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.9/312.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/75.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm=OpenAI()\n",
        "#llm #by default it load the gpt 3.5 turbo model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCopQef68fxg",
        "outputId": "8b4847ba-529a-4319-f5aa-0acb2060dc92"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#temperature parameter\n",
        "#Temperature value - how creative we want our model to be\n",
        "# 0 - > temperature 0, it means model is very safe and it is not takeing any bets\n",
        "# 1 - > it will take risk and it might generate wrong output but the output is very creative\n",
        "llm = OpenAI(temperature = 0.9)"
      ],
      "metadata": {
        "id": "k8oBWQAK81dc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prompt\n",
        "text = \"what would be a good company name for a company that makes colorful socks?\""
      ],
      "metadata": {
        "id": "kVWBP1SaFkdR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.predict(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "BFcW2yi3F-Pu",
        "outputId": "58fa8c02-d724-40a4-c3a5-7aaf0ecd4439"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nRainbow Footwear Co.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.predict(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXBme7EuGYNb",
        "outputId": "1838529c-0124-40d5-8be0-9fe54bdd4be9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Rainbow Toes\" or \"Vibrant Socks Co.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.invoke(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ni9XVx1LGIW_",
        "outputId": "a4e106b8-fe40-409f-a31b-ec304b33b7a9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\"RainbowSocks Co.\" '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4tIzyApGRZy",
        "outputId": "797b6454-a883-4a4e-a1a7-ae877536197b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rainbow Threads \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRZhdc5eGNYM",
        "outputId": "d3f0fad1-961f-4dc6-9d63-0e0310e401af"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\"Kaleidosock Co.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm=OpenAI(temperature=0.2)"
      ],
      "metadata": {
        "id": "an2CBcPYG6lx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"I want to open a car resale shop suggest a fancy name for this\""
      ],
      "metadata": {
        "id": "NZ3JgroYHw54"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(text1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGmO0sdNH99o",
        "outputId": "1bdada1b-bb34-4c4a-97df-4057f0be306a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Revive Auto Boutique\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm=OpenAI(temperature=1)"
      ],
      "metadata": {
        "id": "n1cHmZ9HIAZV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"I want to open a car resale shop suggest a fancy name for this\""
      ],
      "metadata": {
        "id": "N0QoMU0dIMxy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm.invoke(text1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTsXvUAbIPYe",
        "outputId": "85b05725-e5d4-4f39-b37c-6abf122d4d41"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Revamp Rides\n",
            "2. Auto Elegance\n",
            "3. Luxe Auto Exchange\n",
            "4. Elite Wheels Resale\n",
            "5. Velocity Motors\n",
            "6. Prestige Pre-Owned\n",
            "7. The Car Boutique\n",
            "8. Drive & Dazzle\n",
            "9. High-End Auto Emporium\n",
            "10. Glamorous Garage \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hugging face model\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y-30G1cIP7t",
        "outputId": "fd537116-48fa-40ed-bc09-b4fba73d8efe"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "I6IbCpG_I0AZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the hugging face model\n",
        "llm = HuggingFaceHub(repo_id='google/flan-t5-large', model_kwargs={\"temperature\":0.8})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G7DCktiJBbR",
        "outputId": "60787c83-fd56-45ed-b2ae-59db36fd75c3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"translate English to german: how are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "DYmdehqoKC7_",
        "outputId": "eaf1553b-eca1-4bfa-aa2b-4878eb1b8918"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wie sind Sie?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(\"translate English to french: how are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "F2FqSSWMKQjB",
        "outputId": "149561b5-d657-40c7-9f9b-aafb9a0cabf1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'         '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n",
        "\n",
        "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0.9,\"max_length\":64})\n",
        "name = llm.predict('I want to open a car resale shop suggest a fancy name for this')\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TZuzaFxKh3T",
        "outputId": "3371388a-3591-4dd4-ce7a-98151a0023d7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car resale\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n",
        "\n",
        "llm = HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs={\"temperature\":0.9,\"max_length\":64})\n",
        "name = llm.predict('I want to open a car resale shop suggest a fancy name for this')\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4qLjVc9Lkxj",
        "outputId": "f0ca4de4-f3af-4a5a-c0f7-64cf415525db"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to open a car resale shop suggest a fancy name for this business\n",
            "please\n",
            "\n",
            "Need some more information to better include the focus of the business, but here’s a starter list of fancy names!\n",
            "\n",
            "1. Luxe Motorcars\n",
            "2. Avant Motors\n",
            "3. Prestige Autos\n",
            "4. Grand Limo\n",
            "5. Finest Fine Cars\n",
            "6. Riviera Roadsters\n",
            "7. Dream Machines\n",
            "8. Fine Five-O\n",
            "9. Noble Necessities\n",
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n",
        "\n",
        "llm = HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs={\"temperature\":0.9,\"max_length\":64})\n",
        "name = llm.predict('translate english to tamil: how are you?')\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1f2YqjTMHil",
        "outputId": "ac64afb6-2d87-45c9-fe55-e0064d55746e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "translate english to tamil: how are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt templates\n",
        "\n",
        "#import promt template\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = OpenAI(temperatur=0.9)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables = ['cuisine'],\n",
        "    template = 'i want to open a restaurant for {cuisine} food. suggest a fance name for this.'\n",
        ")\n",
        "\n",
        "p = prompt_template_name.format(cuisine=\"italian\")\n",
        "print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "holk-ITzMq8d",
        "outputId": "5184df4f-999f-4666-fc10-2aeb169611da"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i want to open a restaurant for italian food. suggest a fance name for this.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! temperatur is not default parameter.\n",
            "                temperatur was transferred to model_kwargs.\n",
            "                Please confirm that temperatur is what you intended.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = OpenAI(temperatur=0.9)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables = ['cuisine'],\n",
        "    template = 'i want to open a restaurant for {cuisine} food. suggest a fance name for this.'\n",
        ")\n",
        "\n",
        "p = prompt_template_name.format(cuisine={\"italian\",'indian'})\n",
        "print(p)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZbyFPgsOmrx",
        "outputId": "c32d5492-6a74-4a49-bd28-7117ab986eb5"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i want to open a restaurant for {'indian', 'italian'} food. suggest a fance name for this.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! temperatur is not default parameter.\n",
            "                temperatur was transferred to model_kwargs.\n",
            "                Please confirm that temperatur is what you intended.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "#llm = OpenAI(temperatur=0.9)\n",
        "prompt=PromptTemplate.from_template(\"what is a good name for a company that makes {product}\")\n",
        "prompt.format(product='colorful socks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "whVxcScgPOJs",
        "outputId": "3db5d4a0-e1f9-4d3b-dc1d-ebd7577da3e9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what is a good name for a company that makes colorful socks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chains\n",
        "#if the user is giving any input then it goes to the prompt template then it will go inside the LLM\n",
        "#1. user , 2. prompt template, 3. llm - chain of events\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm=OpenAI(temperature=0.9)"
      ],
      "metadata": {
        "id": "pKisc5BVP726"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt = PromptTemplate.from_template('what is a good name for a company that makes {product}')\n",
        "prompt.format(product='colorful socks')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "b3grTOYuQ23g",
        "outputId": "a52365bf-97e6-429d-bac0-36c02380d928"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what is a good name for a company that makes colorful socks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM Chain\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "#prompt template - > LLM -> get llm response\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "response = chain.run('colorful socks')\n",
        "print(response)\n",
        "\n",
        "\n",
        "#here i define my llm first - openAI\n",
        "#i define my prompt template\n",
        "#in the llm chain i will pass the llm and prompt\n",
        "#in the llm chain i also give the input(colorful socks) as a product to the prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPmMDEIfRPTH",
        "outputId": "64648e8e-2ef0-42fd-ee4c-2775bc598ec2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Rainbow Feet Co.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#verbose\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "#prompt template - > LLM -> get llm response\n",
        "chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
        "response = chain.run('car resale')\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpJjXY9NRy5K",
        "outputId": "c27fabae-7e4a-47ab-d6b6-868ace0db30b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mwhat is a good name for a company that makes car resale\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "ReVolve Auto Resale Co. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#using hugging face\n",
        "llm_huggingface = HuggingFaceHub(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\", model_kwargs={\"temperature\":0.9,\"max_length\":64})\n"
      ],
      "metadata": {
        "id": "HmzuS8zLT4Ji"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm_huggingface, prompt=prompt, verbose=True)\n",
        "response = chain.run('car resale')\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zptFVo7CUSRn",
        "outputId": "5025e9b0-8a7b-47c2-b49c-20250bf1eaba"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mwhat is a good name for a company that makes car resale\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "what is a good name for a company that makes car resale easier?\n",
            "\n",
            "Bullet Point Branding\n",
            "\n",
            "* CarExp\n",
            "* DealerDirect\n",
            "* PreOwnedPro\n",
            "* VehicleVerified\n",
            "* WholesaleWheels\n",
            "* BuyBack.com\n",
            "* ReUsedRide\n",
            "* InstantInventory\n",
            "* CurbCrawlers\n",
            "* DealershipDepot\n",
            "* Pre-VettedPre-Owned\n",
            "* TradeInTitle\n",
            "* UsedCarTropolis\n",
            "* GetGoneAutos\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple Sequential chain - when we have more than one prompts and we need to process at a time then we use simple sequential chain\n",
        "\n",
        "#initialize the llm\n",
        "llm = OpenAI(temperature=0.8)\n",
        "\n",
        "#initialize the prompt template\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables = ['cusinie'],\n",
        "    template = 'I want to open a new restaurant for {cuisine} food can you suggest me a good name for this'\n",
        ")\n",
        "\n",
        "name_chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "\n",
        "#we are having another prompt template\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables = ['restaurant_name'],\n",
        "    template = 'can you suggest me some menu items for my {restaurant_name}'\n",
        ")\n",
        "\n",
        "#create a chain for food item prompt\n",
        "food_item_chain = LLMChain(llm=llm, prompt=prompt_template_items)"
      ],
      "metadata": {
        "id": "hvUqm4feUZ0R"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets use the two prompt templates with llm using simple sequential chain\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "chain = SimpleSequentialChain(chains=[name_chain, food_item_chain])"
      ],
      "metadata": {
        "id": "AAlXyynkaos8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content = chain.run('american')\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBVEV7C0cMC7",
        "outputId": "df54e9ec-fc4c-4bc8-987e-5d49276a053e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. \"Classic Cheeseburger with Fries\"\n",
            "2. \"BBQ Pulled Pork Sandwich\"\n",
            "3. \"Philly Cheesesteak\"\n",
            "4. \"Chicken and Waffles\"\n",
            "5. \"Mac and Cheese Bites\"\n",
            "6. \"Buffalo Chicken Wings\"\n",
            "7. \"Apple Pie with Vanilla Ice Cream\"\n",
            "8. \"Fried Chicken Platter\"\n",
            "9. \"BBQ Baby Back Ribs\"\n",
            "10. \"New York Style Cheesecake\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#here it suggesting some food items and not the restaurant name\n",
        "#inorder to get that one we need to use something called \"Sequential chain\"\n",
        "\n",
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables = ['cusinie'],\n",
        "    template = 'I want to open a new restaurant for {cuisine} food can you suggest me a good name for this'\n",
        ")\n",
        "\n",
        "#here we need to mention the extra parameter (output_key) whenever we use sequential chain\n",
        "name_chain = LLMChain(llm=llm, prompt=prompt_template_name, output_key='restaurant_name')\n",
        "\n",
        "\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables = ['restaurant_name'],\n",
        "    template = 'can you suggest me some menu items for my {restaurant_name}'\n",
        ")\n",
        "\n",
        "#here we need to mention the extra parameter (output_key) whenever we use sequential chain\n",
        "food_item_chain = LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1ksA4th8cZvt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sequential chain\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "chain = SequentialChain(\n",
        "    chains =  [name_chain, food_item_chain],\n",
        "    input_variables = ['cuisine'],\n",
        "    output_variables = ['restaurant_name','menu_items']\n",
        ")"
      ],
      "metadata": {
        "id": "msjGdEbrgwUi"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain({'cuisine':'indian'}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxoACwnejok8",
        "outputId": "8318e805-017b-4380-cec9-12fb15e39c93"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'cuisine': 'indian', 'restaurant_name': '\\n\\n1. \"Spice Fusion\"\\n2. \"Taste of India\"\\n3. \"Curry Craze\"\\n4. \"Naan Stop\"\\n5. \"Masala Magic\"\\n6. \"Saffron Bites\"\\n7. \"Chai & Chutney\"\\n8. \"Namaste Kitchen\"\\n9. \"Tandoori Tales\"\\n10. \"Flavors of the East\"\\n11. \"The Indian Kitchen\"\\n12. \"Bollywood Bites\"\\n13. \"Taj Mahal Tastes\"\\n14. \"Samosa Shack\"\\n15. \"Raj Mahal Cuisine\"', 'menu_items': '\\n\\n1. Spice Fusion:\\n- Spicy Chicken Tikka Masala\\n- Chana Masala (chickpea curry)\\n- Lamb Vindaloo\\n- Vegetable Korma\\n- Tandoori Shrimp\\n- Aloo Gobi (potato and cauliflower curry)\\n- Paneer Tikka (grilled cheese cubes in a spicy sauce)\\n- Spiced Basmati Rice\\n- Garlic Naan\\n- Mango Lassi (yogurt drink)\\n\\n2. Taste of India:\\n- Butter Chicken\\n- Saag Paneer (spinach and cheese curry)\\n- Chicken Biryani\\n- Dal Makhani (creamy lentil curry)\\n- Lamb Rogan Josh\\n- Tandoori Chicken\\n- Vegetable Pakoras (fried vegetable fritters)\\n- Garlic Naan\\n- Mango Chutney\\n- Masala Chai (spiced tea)\\n\\n3. Curry Craze:\\n- Beef Madras Curry\\n- Baingan Bharta (roasted eggplant curry)\\n- Chicken Korma\\n- Vegetable Jalfrezi (mixed vegetable curry)\\n- Shrimp Curry\\n- Tandoori Fish\\n- Samosas (fried savory pastries)\\n- Onion Bhajis (spiced onion fritters)\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Agents and tools\n",
        "\n",
        "#example- chatgpt is trained on the data upto the year 2021 or 2022 if you are some questions with related to the year 2023 0r 2024, then it fails to give you the answer\n",
        "\n",
        "#lets say your are building an application this kind of scenario will happen to you then what will your work around? - RAG Approach - gradient application\n",
        "\n",
        "#we can use real time google search engine - search the que in google - ans will give by the google - process the ans with the help of LLM, then show to the user.\n",
        "\n",
        "# wikipedia also we can consider\n",
        "\n",
        "#here we are connecting the agents\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hkzk2hillCw7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3ccEglNs2j5",
        "outputId": "5c6922d1-a7ff-4b1b-a8cd-0d543ef265ca"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2024.2.2)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32004 sha256=dc83474e89e3bd10cc336c55c168e80ba74304f6700f5805ed0b5d8998a70042\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://serpapi.com/\n",
        "import os\n",
        "from google.colab import userdata\n",
        "serp_api_key=userdata.get('serp_api_key')\n",
        "os.environ['SERPAPI_API_KEY']=serp_api_key"
      ],
      "metadata": {
        "id": "XNGIwCdUs8xq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm=OpenAI(temperature=0)\n",
        "\n",
        "tools=load_tools(['serpapi','llm-math'],llm=llm)\n",
        "\n",
        "agent=initialize_agent(tools, llm,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "agent.run(\"what was the GDP of US in 2023?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "aGmlp_DpugYy",
        "outputId": "55c17b7e-555c-4f66-d93f-68c9873516f1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should search for the GDP of US in 2023\n",
            "Action: Search\n",
            "Action Input: \"GDP of US in 2023\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m$27.36 trillion\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use a calculator to convert the GDP from dollars to another currency if needed\n",
            "Action: Calculator\n",
            "Action Input: $27.36 trillion\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 27360000000000\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should check multiple sources to verify the accuracy of the GDP\n",
            "Action: Search\n",
            "Action Input: \"GDP of US in 2023\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m$27.36 trillion\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: $27.36 trillion\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'$27.36 trillion'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm=OpenAI(temperature=0)\n",
        "\n",
        "tools=load_tools(['serpapi','llm-math'],llm=llm)\n",
        "\n",
        "agent=initialize_agent(tools, llm,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "agent.run(\"In the what year was the film LEO with vijay released?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "3yOimXM9wrbS",
        "outputId": "6e657e17-ed1c-440c-ca58-9453a1f85886"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should search for the release year of the film LEO with vijay\n",
            "Action: Search\n",
            "Action Input: \"LEO with vijay release year\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mOctober 18, 2023\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should double check this information\n",
            "Action: Search\n",
            "Action Input: \"LEO with vijay release date\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mOctober 18, 2023\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The film LEO with vijay was released on October 18, 2023.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The film LEO with vijay was released on October 18, 2023.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOuoKGqQxyBO",
        "outputId": "f0c3fb20-4765-47aa-9cf6-4a8dd4cd7d68"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=d08104ca7e0cf2870ddc032dde1741543299b1550b70b8eeaf2b76badcbba7be\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm=OpenAI(temperature=0)\n",
        "#the tool we will give the agent access to, make sure the 'llm-math' tool uses an LLM\n",
        "tools=load_tools(['wikipedia','llm-math'],llm=llm)\n",
        "#finally, let's initialize an agent with the tools, the language model and the type of agent we want to use\n",
        "agent=initialize_agent(tools, llm,agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "agent.run(\"LEO 2023 with vijay movie released date?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "4ReQ-THIxkRw",
        "outputId": "5f6faad2-eaa4-40ca-878d-0ff6fb2daad1"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use Wikipedia to find information about the film LEO with vijay\n",
            "Action: Wikipedia\n",
            "Action Input: LEO with vijay\u001b[0m\n",
            "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find information about the film LEO with vijay\n",
            "Action: Wikipedia\n",
            "Action Input: LEO with vijay film\u001b[0m\n",
            "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find information about the film LEO with vijay\n",
            "Action: Wikipedia\n",
            "Action Input: LEO (film)\u001b[0m\n",
            "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find information about the film LEO with vijay\n",
            "Action: Wikipedia\n",
            "Action Input: LEO (2016 film)\u001b[0m\n",
            "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find information about the film LEO with vijay\n",
            "Action: Wikipedia\n",
            "Action Input: LEO (2017 film)\u001b[0m\n",
            "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find information about the film LEO with vijay\n",
            "Action: Wikipedia\n",
            "Action Input: LEO (2018 film)\u001b[0m\n",
            "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find information about the film LEO with vijay\n",
            "Action: Wikipedia\n",
            "Action Input: LEO (2019 film)\u001b[0m\n",
            "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find information about the film LEO with vijay\n",
            "Action: Wikipedia\n",
            "Action Input: LEO (2020 film)\u001b[0m\n",
            "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find information about the film LEO with vijay\n",
            "Action: Wikipedia\n",
            "Action Input: LEO (2021 film)\u001b[0m\n",
            "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use Wikipedia to find information about the film LEO with vijay\n",
            "Action: Wikipedia\n",
            "Action Input: LEO (2022 film)\u001b[0m\n",
            "Observation: Wikipedia is not a valid tool, try one of [wikipedia, Calculator].\n",
            "Thought:\u001b[32;1m\u001b[1;3m I should use Calculator to find the release year of the film LEO with vijay\n",
            "Action: Calculator\n",
            "Action Input: 2022 - 1\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2021\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The film LEO with vijay was released in 2021.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The film LEO with vijay was released in 2021.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Memory\n",
        "#example - chat gpt - give me a code for add 2 numbers\n",
        "# give me code without function\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)"
      ],
      "metadata": {
        "id": "s4v-GnPMx7Ly"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
        ")"
      ],
      "metadata": {
        "id": "R5g7qVn_J1cs"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
        "name = chain.run('Mexican')\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxDfIRlvKOQX",
        "outputId": "01d73bff-b3e8-4047-9252-301a802dc6a3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"El Sabor de México\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = chain.run('South indian')\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "627OwjlIKj3K",
        "outputId": "1174315c-b109-4c9c-8650-f49ceb10b73e"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Spice House South\" or \"Southern Spice Kitchen\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(chain.memory) #it is not saving or remembering anything"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqMTmvQCKq_x",
        "outputId": "32d1bd00-83b1-4832-8e1a-ac5bfab5abcd"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NoneType"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving or remembering\n",
        "#Conversation Buffer Memory\n",
        "\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
        "name = chain.run('Mexican')\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D_ljVU9Kyng",
        "outputId": "773b8b07-fa71-42b9-b441-e9fe676548af"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"Casa de Sabores\" (House of Flavors)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = chain.run('south indian')\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sshiANzLk2a",
        "outputId": "9f259354-0ecc-45da-82bf-9047b84d74a0"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Savor Spice - South Indian Kitchen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = chain.run('italian')\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nuf6nC_MGcz",
        "outputId": "65912a34-2ec8-4c9c-d305-6cbd23d96e56"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\"La Bella Trattoria\" \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = chain.run('US')\n",
        "print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbgCpj7oMLRH",
        "outputId": "3e749e51-3006-4b36-e575-bbd72539de87"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\"Stateside Eats\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7WoTNwzLo_2",
        "outputId": "46d62a0e-cdfe-4f0e-c03b-56603f02c46c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Mexican\n",
            "AI: \n",
            "\n",
            "\"Casa de Sabores\" (House of Flavors)\n",
            "Human: south indian\n",
            "AI: \n",
            "\n",
            "\n",
            "Savor Spice - South Indian Kitchen\n",
            "Human: italian\n",
            "AI: \n",
            "\n",
            "\"La Bella Trattoria\" \n",
            "Human: US\n",
            "AI: \n",
            "\"Stateside Eats\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#conversation chain\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
        "print(convo.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo4dfdnsNGid",
        "outputId": "5c275d4e-4151-4f19-bc53-bd2dbd3a1d7f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run('which team won the first cricket world cup?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "HLzz94OENhgd",
        "outputId": "30f1c7e8-1326-4800-c913-786ef973acd7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The first cricket world cup was held in 1975 and was won by the West Indies team. They defeated Australia in the final by 17 runs. The team was led by Clive Lloyd and had notable players such as Viv Richards, Gordon Greenidge, and Michael Holding. It was a historic moment for the West Indies as it was their first ever world cup victory.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lets iam asking out of context question\n",
        "convo.run('total of 5+5?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "SVpOgyg9Nxoc",
        "outputId": "985ac5d6-ed1c-4390-fee7-f6c7f1bf224c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  The total of 5+5 is 10. This is a simple addition problem that can be easily solved by adding the two numbers together. In mathematics, addition is a basic arithmetic operation that represents combining two or more quantities into a larger quantity. It is often represented by the symbol \"+\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Again i ask the question with related to cricket\n",
        "convo.run('who is the highest wicket taker in that match?') #here it is remembering the previous context"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZfEAZ25eN_ki",
        "outputId": "2ec2c228-9760-429d-c5cc-adb9468ea6ee"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The highest wicket taker in the 1975 cricket world cup final was Andy Roberts from the West Indies team. He took 3 wickets for 32 runs in his 12 overs. Roberts was a key contributor to the West Indies' victory and was known for his fast bowling abilities. He ended the tournament with a total of 11 wickets, making him the highest wicket taker of the entire tournament.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the issuw with the buffer memory is whatever we ask for many time its get added in the memory\n",
        "#conversation buffer window memory\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "\n",
        "convo = ConversationChain(\n",
        "    llm=OpenAI(temperature=0.7),\n",
        "    memory=memory\n",
        ")\n",
        "convo.run(\"Who won the first cricket world cup? \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "PHyu6YgCLvl8",
        "outputId": "efa3a73c-476a-4a78-a460-0153d06e6d7c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The first cricket world cup was held in 1975 and was won by the West Indies. The tournament was organized by the International Cricket Council and featured eight teams, including Australia, England, India, Pakistan, New Zealand, Sri Lanka, and East Africa. The final match was held at Lord's Cricket Ground in London, where the West Indies defeated Australia by 17 runs to claim the championship title. This was the first time a World Cup was held for the sport of cricket and it has since become one of the most popular international sporting events.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run('value of 20+6?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ngZ8NYcxO1dQ",
        "outputId": "a5db3f82-7151-4b69-b448-83f8a7ac70a8"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The value of 20+6 is 26.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run('who is the highest wicket taker in that match?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5ja3FxJZPEnA",
        "outputId": "ce107860-b449-4ed4-d28c-6b952a560f7e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' I am not sure which match you are referring to. Can you provide more context?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=2)\n",
        "\n",
        "convo = ConversationChain(\n",
        "    llm=OpenAI(temperature=0.7),\n",
        "    memory=memory\n",
        ")\n",
        "convo.run(\"which team won the first cricket world cup? \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "KAJEyUdaPKRI",
        "outputId": "b8fe1697-d005-452b-d4fe-3526fc252df7"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The first cricket world cup was held in 1975 in England. The winning team was the West Indies, beating Australia in the final by 17 runs. The West Indies team was led by Clive Lloyd and had notable players like Viv Richards, Gordon Greenidge, and Andy Roberts. It was a historic moment for the West Indies, as it was the first time they had won the cricket world cup. Do you have any other questions about the first cricket world cup?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run('value of 27+20?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "B8zAIeKTPSji",
        "outputId": "f79d9439-39e6-42ce-b19f-1e00e8935c64"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The value of 27+20 is 47. Would you like me to perform any other calculations for you?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run('who is the highest wicket taker in that match?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_hU9PL5iPWFQ",
        "outputId": "36029f2d-585c-4ee4-ced0-89ce1a07a50e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The highest wicket taker in that match was Gary Gilmour from the Australian team. He took 5 wickets in the final match against the West Indies. Interestingly, he was not even a regular player on the Australian team and was only brought in as a replacement for an injured player. Would you like to know any other details about the players in the first cricket world cup?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz8a1lH-RQ46",
        "outputId": "8385f23b-f644-4793-b80c-be5cdd952f4c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.11.0)\n",
            "Installing collected packages: pypdf\n",
            "Successfully installed pypdf-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Loader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader('/content/OPEN AI.pdf')\n",
        "pages = loader.load()\n"
      ],
      "metadata": {
        "id": "mriy_k1oPirx"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YO0wFbxjRZfT",
        "outputId": "74d8814a-d2a5-46b3-eee3-3eb33afcdbb4"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='OPEN AI  \\nStep 1: Login in to your account  \\n \\nClick on the API  \\n \\n \\nThings to install when we want  to use openapi’s  \\n1. Anaconda  \\n2. Python  \\n3. Jupyter notebook  \\n4. Create virtual env and activate the env  \\n5. Install the dependencies or required packages  (open ai, ……)  \\nOpen anaconda prompt and create a new environment  \\n \\n \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 0}),\n",
              " Document(page_content='Environment created  \\n \\nNow we need to activate the environment  \\n \\nPip list  \\n \\nInstall the Jupyter notebook  \\n \\n \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 1}),\n",
              " Document(page_content='Open Jupyter notebook and install the open AI library  \\n \\nGenerate OpenAI API key: \\n \\nOnce you generated the openAPI key, then open the OpenAI playground  \\nhttps://platform.openai.com/playground  \\nin the assistants tab you will see the two options like Assistants and chat  \\n \\nThere you will find three options  \\n1. System  \\n2. User  \\n3. Model  \\nSystem – How your model is going to behave or behavior of the system. So here we need to set the \\nbehavior of the system.  \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 2}),\n",
              " Document(page_content=' \\n \\nTemperature:  \\nIf you set the higher value of temperature which means it indicates that give me a more creative answer  \\nand adding randomness  \\nIf the value is zero then it indicates that gives the straight forward answer.  \\nMaximum length – Token length  \\nTop – Adding diversity inside our output  \\nFrequency penalty – if we don’t want to repeat the tokens inside our output  then we need to mention \\nthe frequency penalty.  \\n \\nClick view code  \\nWe will get the entire python code  \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 3}),\n",
              " Document(page_content=' \\nNow we change the behavior the system.  \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 4}),\n",
              " Document(page_content=' \\nSee the differences in the output while we change the behavior of the system.  \\nAssistants  in open ai playground : \\n \\nWhile discuss the assistant’s  part in a detailed way while we implement in the project section.  \\n \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 5}),\n",
              " Document(page_content='Pypi  Open AI  \\n \\nOpen AI tokens  \\nhttps://platform.openai.com/tokenizer  \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 6}),\n",
              " Document(page_content=' \\nPricing  \\nhttps://openai.com/pricing  \\n \\n \\n \\n \\n \\n \\n \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 7})]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gPQuNeaRbwF",
        "outputId": "6523bbeb-309c-425a-f577-6a833a120e83"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='OPEN AI  \\nStep 1: Login in to your account  \\n \\nClick on the API  \\n \\n \\nThings to install when we want  to use openapi’s  \\n1. Anaconda  \\n2. Python  \\n3. Jupyter notebook  \\n4. Create virtual env and activate the env  \\n5. Install the dependencies or required packages  (open ai, ……)  \\nOpen anaconda prompt and create a new environment  \\n \\n \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 0}), Document(page_content='Environment created  \\n \\nNow we need to activate the environment  \\n \\nPip list  \\n \\nInstall the Jupyter notebook  \\n \\n \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 1}), Document(page_content='Open Jupyter notebook and install the open AI library  \\n \\nGenerate OpenAI API key: \\n \\nOnce you generated the openAPI key, then open the OpenAI playground  \\nhttps://platform.openai.com/playground  \\nin the assistants tab you will see the two options like Assistants and chat  \\n \\nThere you will find three options  \\n1. System  \\n2. User  \\n3. Model  \\nSystem – How your model is going to behave or behavior of the system. So here we need to set the \\nbehavior of the system.  \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 2}), Document(page_content=' \\n \\nTemperature:  \\nIf you set the higher value of temperature which means it indicates that give me a more creative answer  \\nand adding randomness  \\nIf the value is zero then it indicates that gives the straight forward answer.  \\nMaximum length – Token length  \\nTop – Adding diversity inside our output  \\nFrequency penalty – if we don’t want to repeat the tokens inside our output  then we need to mention \\nthe frequency penalty.  \\n \\nClick view code  \\nWe will get the entire python code  \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 3}), Document(page_content=' \\nNow we change the behavior the system.  \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 4}), Document(page_content=' \\nSee the differences in the output while we change the behavior of the system.  \\nAssistants  in open ai playground : \\n \\nWhile discuss the assistant’s  part in a detailed way while we implement in the project section.  \\n \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 5}), Document(page_content='Pypi  Open AI  \\n \\nOpen AI tokens  \\nhttps://platform.openai.com/tokenizer  \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 6}), Document(page_content=' \\nPricing  \\nhttps://openai.com/pricing  \\n \\n \\n \\n \\n \\n \\n \\n', metadata={'source': '/content/OPEN AI.pdf', 'page': 7})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vewNn4QARgBl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}